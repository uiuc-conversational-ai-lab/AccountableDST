<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Accountability Modeling of DST">
  <meta property="og:title" content="Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://uiuc-conversational-ai-lab.github.io/AccountableDST/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AccountableDST</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size:30px">Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sites.google.com/view/suvodip-dey" target="_blank">Suvodip Dey</a></sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Yi-Jyun Sun</a></sup>,</span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/gokhan" target="_blank">Gokhan Tur</a></sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/dilek" target="_blank">Dilek Hakkani-Tür</a>
                  </span>
                  </div>

                  <img alt="Conversational AI Lab" src="static/images/ConvAIText.png" style="width:8%" />

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block"><a href="https://uiuc-conversational-ai-lab.github.io/" target="_blank">ConvAI Lab</a><br></span>
                    
                  </div>  -->
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">ConvAI Lab, University of Illinois at Urbana-Champaign<br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2501.10316" target="_blank"
                        class="external-link button is-small is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/uiuc-conversational-ai-lab/Accountable-DST" target="_blank"
                    class="external-link button is-small is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Blog link -->
                <!-- <span class="link-block">
                  <a href="https://vardhandongre.notion.site/ReSpAct-Towards-Building-Large-Language-Model-Based-Conversational-AI-Agents-1342e61c204c8027a631e7427e570e61?pvs=4" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-book"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span> -->
              
              <!-- Notebook LM link -->
              <!-- <span class="link-block">
                <a href="https://notebooklm.google.com/notebook/5caa79d2-1bfb-46e0-8dbb-fbb997a5761a/audio" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-headphones"></i>
                </span>
                <span>Notebook LM</span>
              </a>
            </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Audio Player Section -->
<!-- <section class="section is-small">
  <div class="container has-text-centered">
      <h2 class="title is-4">Listen to Our Project Overview</h2>
      <audio controls class="custom-audio-player">
          <source src="static/audio/respact.wav" type="audio/wav">
          Your browser does not support the audio element.
      </audio>
  </div>
</section> -->

<!-- Audio Player Section -->
<!-- <section style="text-align: center; padding: 20px; background-color: #f9f9f9; border-radius: 8px; max-width: 400px; margin: 0 auto;">
  <h2 style="color: #333; margin-bottom: 1rem;">Listen to Our Project Overview</h2>
  <audio controls style="width: 80%; max-width: 300px; margin: 10px auto;">
    <source src="static/audio/respact.wav" type="audio/wav">
      Your browser does not support the audio element.
  </audio>
</section> -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent LLMs have enabled significant advancements for conversational agents. However, they are also well-known to hallucinate, i.e., they often produce responses that seem plausible but are not factually correct. On the other hand, users tend to over-rely on LLM-based AI agents; they accept the AI’s suggestion even when it is wrong. Adding good friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head, which functions as a binary classifier to predict the slots of the dialogue states. We perform our experiments with three backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented datasets (MultiWOZ and Snips). Our empirical findings demonstrate that this approach not only enables reliable estimation of AI agent errors but also guides the LLM decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy by incorporating accountability heads in modern LLMs for the MultiWOZ dataset. We also show that this method enables the agent to self-correct its actions, further boosting its performance by 3%. Finally, we discuss the application of accountability modeling to prevent user overreliance by introducing friction.

          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- End paper abstract -->


<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conversational Agents: Why They Matter?</h2>
        <div class="content has-text-justified">
          <p>
            In an increasingly complex digital landscape, conversational agents offer a way to bridge the gap between human needs and machine capabilities. These agents can assist us in navigating digital interfaces, support decision-making, and help accomplish goals that require contextual understanding.
          </p>
          <figure class="image is-inline-block" style="width: 70%;"> 
            <img src="static/images/framework2.png" alt="MY ALT TEXT"/>
          <p>
            <b>Existing approaches operate in a one-sided way</b>—executing commands without checking back for clarification. This approach can lead to errors and unmet expectations, especially in cases where precise details matter. However, with conversational agents, there is an added layer of communication, creating a back-and-forth that mirrors human conversation and allows users to guide the agent toward optimal outcomes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overreliance on Task-Oriented Conversational AI</h2>
        <div class="content has-text-justified">
          <p>
            Conversational agents have reached remarkable advancements with the advent of large language models (LLMs). Despite their impressive capabilities, these models frequently suffer from the issue of hallucinations, where they generate information that is incorrect or not grounded in reality. Moreover, users often tend to over-rely on LLM-based AI agents, accepting the AI's suggestions even when they are erroneous. In the context of task-oriented conversations, such overreliance can lead to incorrect or incomplete task execution, thereby undermining the system's reliability. This work explores accountability modeling to prevent overreliance on task-oriented conversational AI.
          </p>
          <!-- <figure class="image is-inline-block"> 
            <img src="static/images/framework2.png" alt="MY ALT TEXT"/>
          </figure>
          <p>
            <b>Existing approaches operate in a one-sided way</b>—executing commands without checking back for clarification. This approach can lead to errors and unmet expectations, especially in cases where precise details matter. However, with conversational agents, there is an added layer of communication, creating a back-and-forth that mirrors human conversation and allows users to guide the agent toward optimal outcomes.
          </p> -->
        </div>     
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">How Accountability Modeling can prevent Overreliance?</h2>
        <div style="clear: both;">
          <div>
            <p>This work proposes an LLM-based generative accountability modeling for task-oriented conversations. In AI, accountability typically refers to the model's capability to explain or justify its action. The main idea of our approach is to add an accountability head in backbone LLMs, which is nothing but a binary classifier for predicting the slots in the dialogue state. The classifier is added to the final token of the dialogue context. The resultant model is jointly trained on the standard language modeling loss and the auxiliary slot classification loss.

            </p>
          </div>
          <div style="float: right; margin-right 1em; width: 40%;">
            <img src="static/images/motivation.png" alt="">
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Why ReSpAct?</h2>
        <div class="content has-text-justified">
          <p>
            ReSpAct, or "Reason, Speak, Act," is a novel framework that builds on the ReAct approach to make interactions more human-centered. ReSpAct takes agent systems a step further by embedding conversations directly into the decision-making process, enabling agents to act autonomously while remaining responsive to user needs. For example, if an agent faces ambiguity, it can proactively ask, "Should I look for a blue sweater in the mens section, or do you have another preference?". It combines reasoning with conversational prompts to allow agents to clarify uncertainties, seek user feedback, and adapt strategies dynamically. This human-in-the-loop approach not only reduces mistakes but allows building user trust and satisfaction by keeping them engaged in real-time.
          </p>
          <figure class="image is-inline-block"> 
            <img src="static/images/examples.png" alt="MY ALT TEXT"/>
          </figure>
          <p>
            The ReSpAct framework empowers LLM-based Agents to execute tasks more accurately by blending autonomous reasoning with natural language interaction. When an agent is unclear on a user request, it can ask a question or share its reasoning, preventing it from acting based on assumptions. This dynamic engagement means fewer misunderstandings and more effective completion of tasks, as seen in tasks like booking accommodations or navigating e-commerce platforms.
          </p>
        </div>     
      </div>
    </div>
  </div>
</section> -->

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="results-carousel" class="carousel results-carousel">
            
            <div class="item">
              <img src="static/images/framework.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue.
              </h2>
            </div>

            <div class="item">
              <img src="static/images/efficient_respact_v2.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                Comparison of (a) ReAct and (b) ReSpAct to solve a game in AlfWorld
              </h2>
            </div>

            <div class="item">
              <img src="static/images/multiwoz.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                Comparison of (a) ReAct and (b) ReSpAct to set up a travel booking in MultiWOZ 
              </h2>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->

 <!-- Takeaway line -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">ReSpAct synergistically combines the essential skills for building task-oriented "conversational" agents. Agent can reasong through thoughts, interact with the user, and take actions in the environment to solve tasks.</h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">How Accountability Modeling can prevent Overreliance?</h2>
        <p>          
          Task-oriented dialogue systems (TODS) are designed to assist users in completing a task or goal through conversations. Dialogue state tracking (DST) is a crucial component of TODS that accounts for understanding the user intentions and keeping track of the dialogue state. Task-oriented dialogues are sensitive to DST errors (false positives and false negatives), as a single error can significantly change the course of the conversation. For example, in Figure 1, <i>attraction-area</i> is a false negative prediction. As a result, the user may overrely on the AI's suggestion and end up booking a park that is not near or centre of the town. Such problems deteriorate the user experience in real-world conversations.
        </p>     
        <br>   
        <div class="has-text-centered"> 
          <figure class="image is-inline-block" style="width: 60%"> 
            <img src="static/images/amd_motivation.png" alt="MY ALT TEXT"/>
            <figcaption>Figure 1: Overview of Accountability Modeling. The model simultaneously generates the dialogue state and estimates slot probabilities (of being included in the dialogue state), then detects potential errors using the estimated slot probabilities. The errors can either be self-corrected or confirmed by the user via friction turns.</figcaption>
          </figure>
        </div>
        <br>
        <p>In this work, we tackle overreliance using accountability modeling of DST that can detect the dialogue state errors in advance and rectify them. For example, in Figure 1, <i>attraction-area</i> is not in the prediction, the accountability model is able to detect it and self-corrects the predicted dialogue state. Instead of self-correcting the errors, the model can also introduce such friction turns (like confirmation about model uncertainty and errors), which helps to rectify the error in subsequent turns and prevent overreliance.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Architecture</h2>
        <p>          
          The main idea of our approach is to add an accountability head in backbone LLMs, which is nothing but a binary classifier for predicting the slots in the dialogue state. The classifier is added to the final token of the dialogue context. The resultant model is jointly trained on the standard language modeling loss and the auxiliary slot classification loss. The accountability head helps to estimate the slot probabilities for all the slots, which can be used to detect the false positive and false negative slots in the predicted dialogue state. Furthermore, the inclusion of the accountability head acts as an auxiliary loss that helps in the learning of dialogue state generation. 
        </p>        
        <br>
        <div class="has-text-centered"> 
          <figure class="image is-inline-block" style="width: 75%"> 
            <img src="static/images/arc.png" alt="MY ALT TEXT"/>
            <figcaption>Figure 2: Model architecture of the LLM-based generative accountability modeling for DST.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dialogue State Tracking Performance</h2>
        <p>          
          We perform our experiments with three backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented datasets (MultiWOZ and Snips). Our empirical findings (shown in Figure 3) demonstrate that this approach not only enables reliable estimation of AI agent errors but also guides the LLM decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy by incorporating accountability heads in modern LLMs for the MultiWOZ dataset. We also show that this method enables the agent to self-correct its actions, further boosting its performance by 3%.
        </p>     
        <br>   
        <div class="has-text-centered"> 
          <figure class="image is-inline-block"> 
            <img src="static/images/amd_result.png" alt="MY ALT TEXT"/>
            <figcaption>Figure 3: Comparison of the DST performance on the MultiWOZ 2.4 and Snips test datasets with different LLM backbones. The relative JGA improvement of proposed M<sub>AMD</sub> (accountability modeling) and M<sub>AMD+SC</sub> (accountability modeling with self-correction), compared to the respective M<sub>SFT</sub> (supervised finetuning) baseline, is highlighted in blue. The best results are indicated in bold font.</figcaption>
          
          </figure>
        </div>
        <div class="has-text-centered"> 
          <figure class="image is-inline-block" style="width: 50%"> 
            <img src="static/images/amd_result_compare.png" alt="MY ALT TEXT"/>
            <figcaption>Figure 4: JGA comparison between various DST models on the MultiWOZ 2.4 test data.</figcaption>
          
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dialogue State Correction using Accountability Modeling</h2>
        <p>          
          Figure 5 shows illustrative examples of the model prediction from the MultiWOZ and Snips datasets. In the first example, the model detected a false positive slot (<i>restaurant-pricerange</i>) and filtered it to rectify the prediction. The second example contains a false negative slot (<i>attraction-type</i>), which is corrected successfully. In the third example, the model detects both false positive and false negative slots and successfully rectifies them. In the fourth example, the original prediction of the model is correct. However, self-correction adds an extra slot (<i>train-departure</i>), which makes the prediction wrong. The fifth one shows an instance where the algorithm partially corrects an error. The last two examples are from the Snips, where the algorithm successfully rectifies the prediction. 
        </p>    
        <br>    
        <div class="has-text-centered"> 
          <figure class="image is-inline-block" style="width: 90%"> 
            <img src="static/images/acc_example.png" alt="MY ALT TEXT"/>
            <figcaption>Figure 5: Illustrative example of dialogue state corrections.</figcaption>
           
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Performance</h2>
        <div class="content has-text-justified">
          <p>
            We test ReSpAct in 3 settings: AlfWorld (Embodied Decision-Making), WebShop (Web Agent Decision-Making) and MultiWOZ (Task Oriented Dialogue System). Each of these environments presents unique challenges, requiring the agent to balance reasoning, dialogue, and action in different ways. To evaluate ReSpAct effectively, we design human-annotated trajectories with sparse occurences of reasoning traces and dialogs, allowing the language model to autonomously decide when to "think," "speak," or "act." This design simulates a real-world scenario where agents must not only follow instructions but also navigate uncertainties and dynamically adapt to the users needs.
          </p>
        </div>   
    <h2 class="title is-3"> 
      AlfWorld 
    </h2>
    <p>
      We evaluate ReSpAct in AlfWorld, a synthetic environment based on the TextWorld framework and aligned with the embodied ALFRED benchmark. AlfWorld presents six task categories, such as finding hidden objects, moving items, and using objects together (e.g., heating a potato in a microwave). In this setting, ReSpAct enables agents to ask contextually relevant questions, provide status updates, and seek clarification to navigate tasks more effectively. Compared to a baseline ReAct agent, which relies on internal reasoning without user feedback, ReSpAct demonstrates notable improvements. It achieves an 87.3% success rate across tasks, outperforming ReAct's 80.6% by effectively integrating dialogue into its decision-making process.
    </p>
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/alfworld-respact.mp4"
        type="video/mp4">
      </video>
      <img src="static/images/table-alfworld.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison of (a) ReAct and (b) ReSpAct over Task-specific success rates (%) in Alfworld
      </h2>
      <figure class="image is-inline-block"> 
        <img src="static/images/sankey.png" alt="MY ALT TEXT"/>
      </figure>
      <p>
        In our analysis we observed how ReSpAct changes the way agents handle tasks. While ReAct agents tend to jump straight into actions, ReSpAct introduces more "thinking" and "speaking" moments. The ReSpAct agents show a sharp reduction in "invalid actions"—down to just 3% from ReAct's 13%. Invalid actions are wasted steps, like trying to open an already open door, that dont contribute to task success.
      </p>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">
      MultiWOZ
      </h2>
      <p>
        We evaluate ReSpAct in a MultiWOZ environment, where the agent assists with booking hotels, restaurants, and taxis by understanding user preferences, asking clarifying questions, and making informed decisions. Compared to a ReAct baseline that operates without user feedback, ReSpAct achieves higher scores on two metrics: Inform Rate, which checks if all requested information (e.g., hotel address and price) was correctly provided, and Success Rate, which further requires that all user goals, like completing a booking, are fully met. Using GPT-4o-mini, ReSpAct scores 72.2% on Inform and 51.8% on Success, compared to the baseline 66.7% and 48.8%, respectively. 
      </p>
      <br>
    <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      <video poster="" id="tree" autoplay controls muted loop style="max-width: 100%; height: auto;">

        <source src="static/videos/respact-demo-woz-2.mov" type="video/mp4">
      </video>
      <img src="static/images/table-woz.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison of Inform and Success scores for MultiWOZ using GPT-4o-mini and Llama-405B-instruct models.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">
      WebShop
    </h2>
    <p>
      We evaluate ReSpAct in a WebShop environment, where the agent navigates a simulated online shopping platform containing over 1.18 million real-world products to identify and purchase items based on user preferences. The agent actively searches for products matching user requirements, asks clarifying questions to refine search results, and makes informed purchasing decisions. To assess performance, we use two metrics: (1) average score, which reflects the percentage of desired attributes met by the chosen product across all episodes, and (2) success rate, which measures the percentage of episodes where the chosen product fully satisfies the users requirements. Our results show that ReSpAct achieves an average score of 32.7% and a success rate of 12% with a user simulator. With human interaction, ReSpAct reaches an 85.8% average score and 50% success rate, underscoring the critical role of interactive questioning and feedback in enhancing accuracy and user satisfaction with task-oriented web shopping agents.
    </p>
    <br>
    <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/respact-demo-webshop.mov"
        type="video/mp4">
      </video>
      <img src="static/images/table-shop.png" alt="MY ALT TEXT" style="width: 50%;"/>
      <h2 class="subtitle has-text-centered">
        Comparison of Score and success rate (SR) on 100 Test WebShop trajectories using GPT-4o-mini
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Implications and the Future of Conversational Agents</h2>
    <p>
      By embedding conversational checkpoints within its reasoning process, ReSpAct addresses longstanding issues like error propagation, task ambiguity, and rigid agent behavior. In interactive environments like WebShop and AlfWorld, ReSpAct's approach not only reduces errors during task completion but also enables agents to align more closely with user intent by actively involving users in decision-making. This responsiveness transforms the agent into a collaborative partner rather than a passive tool.
    </p>
  </section> -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{dey2025preventingoverreliancetaskorientedconversational,
        title={Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling}, 
        author={Suvodip Dey and Yi-Jyun Sun and Gokhan Tur and Dilek Hakkani-Tur},
        year={2025},
        eprint={2501.10316},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2501.10316},
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Acknowledgements -->
<section class="section" id="Acknowledgements">
<div class="container is-max-desktop content">
  <h2 class="title">Acknowledgements</h2>
  <p>
    This work was supported in part by Other Transaction award HR0011249XXX from the U.S. Defense Advanced Research Projects Agency (DARPA) Friction for Accountability in Conversational Transactions (FACT) program and has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program, through which leading foundation models hosted by Microsoft Azure and access to Azure credits were provided to conduct the research.
  </p>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website's template is borrowed from <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. We thank the authors for open-sourcing their code.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
